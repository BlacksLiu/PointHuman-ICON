{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob \n",
    "import os \n",
    "subject_paths = sorted(glob(\"/mnt/local4T/pengfei/projects/PointHuman/PointHuman-ICON/data/cape_3views/*\"))\n",
    "pred_root = \"/mnt/local4T/pengfei/projects/PointHuman/PointHuman-ICON/results/icon-nofilter/cape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pred dir path \n",
    "pred_paths = {\n",
    "    'est_mesh_0.obj': [],\n",
    "    'est_mesh_120.obj': [],\n",
    "    'est_mesh_240.obj': []\n",
    "}\n",
    "for path in subject_paths:\n",
    "    for key in pred_paths.keys():\n",
    "        subject_name = os.path.basename(path)\n",
    "        pred_scan_path = os.path.join(pred_root, f\"{subject_name}/{key}\")\n",
    "        if os.path.exists(pred_scan_path):\n",
    "            pred_paths[key].append(pred_scan_path)\n",
    "\n",
    "for key in pred_paths.keys():\n",
    "    print(pred_paths[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gt obj path\n",
    "gt_scans_root = \"/mnt/local4T/pengfei/projects/PointHuman/PointHuman-ICON/data/cape/scans\"\n",
    "gt_scan_paths = []\n",
    "for path in subject_paths:\n",
    "    subject_name = os.path.basename(path)\n",
    "    gt_scan_path = os.path.join(gt_scans_root, f\"{subject_name}.obj\")\n",
    "    if os.path.exists(gt_scan_path):\n",
    "        gt_scan_paths.append(gt_scan_path)\n",
    "# gt_scan_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection pcls and g.t. mesh \n",
    "# load gt mesh and proj\n",
    "import os \n",
    "from lib.renderer.mesh import load_scan\n",
    "import numpy as np\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import lib.renderer.opengl_util as opengl_util\n",
    "from lib.renderer.gl.init_gl import initialize_GL_context\n",
    "from lib.renderer.gl.color_render import ColorRender\n",
    "from lib.renderer.camera import Camera\n",
    "egl = True\n",
    "\n",
    "# two pointclouds chamfer distance \n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.loss.point_mesh_distance import _PointFaceDistance\n",
    "# from pytorch3d.loss import chamfer_distance\n",
    "import torch \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from pytorch3d.ops.points_normals import estimate_pointcloud_normals\n",
    "from pytorch3d.structures import Pointclouds\n",
    "\n",
    "def estimate_normals(xyzs):\n",
    "    \"\"\"Estimate point normals. The pointcloud should be a sphere-like shape.\n",
    "\n",
    "    Args:\n",
    "        xyzs (tensor): Nx3 or BxNx3.\n",
    "    \"\"\"\n",
    "    no_batch_dim = False\n",
    "    if xyzs.ndim == 2:\n",
    "        xyzs = xyzs[None, ...]\n",
    "        no_batch_dim = True\n",
    "    normals = estimate_pointcloud_normals(\n",
    "        xyzs * 1000.,\n",
    "        neighborhood_size=6,\n",
    "        disambiguate_directions=False,\n",
    "        use_symeig_workaround=True\n",
    "    )\n",
    "    normals = normals / (\n",
    "        torch.linalg.norm(normals, dim=-1, keepdim=True) + 1e-8\n",
    "    )\n",
    "    return normals\n",
    "\n",
    "def test_normals(normals):\n",
    "    no_batch_dim = True\n",
    "    outside_direcion = torch.tensor([0,0,1]).to(device=normals.device)\n",
    "    \n",
    "    normals = torch.sign(\n",
    "        torch.sum(normals * outside_direcion, dim=-1, keepdim=True)) * normals\n",
    "    \n",
    "    normals = (normals + 1.0) / 2 \n",
    "\n",
    "    if no_batch_dim:\n",
    "        normals = normals[0]\n",
    "\n",
    "    return normals\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OrthographicCameras,\n",
    "    PointsRasterizationSettings,\n",
    "    PointsRenderer,\n",
    "    PointsRasterizer,\n",
    "    AlphaCompositor,\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "R, T = look_at_view_transform(20, 0, 0)\n",
    "cameras = OrthographicCameras(device=device, R=R, T=T)\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "    image_size=512, \n",
    "    radius = 0.003,\n",
    "    points_per_pixel = 10\n",
    ")\n",
    "\n",
    "rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "renderer = PointsRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    compositor=AlphaCompositor(background_color=(0, 0, 0))\n",
    ")\n",
    "\n",
    "\n",
    "import trimesh\n",
    "def get_chamfer_distance(gt_pcls, pred_pcls):\n",
    "    gt_pcls = torch.from_numpy(gt_pcls).float()\n",
    "    pred_pcls = torch.from_numpy(pred_pcls).float()\n",
    "    tgt_points = Pointclouds(torch.unsqueeze(gt_pcls,0))\n",
    "    pred_points = Pointclouds(torch.unsqueeze(pred_pcls, 0))\n",
    "    tgt_points=tgt_points.to(device)\n",
    "    pred_points=pred_points.to(device)\n",
    "    chamfer_dist = chamfer_distance(tgt_points, pred_points)[0] * 100\n",
    "\n",
    "    return chamfer_dist.cpu()\n",
    "\n",
    "def get_depth_map(vertices, faces, normals, faces_normals, output_path=None, size=512):\n",
    "    initialize_GL_context(width=size, height=size, egl=egl)\n",
    "\n",
    "    cam = Camera(width=size, height=size)\n",
    "    ortho_ratio = 1 * (512 / size)\n",
    "    cam.ortho_ratio = ortho_ratio\n",
    "    cam.near = -1\n",
    "    cam.far = 1\n",
    "    cam.sanity_check()\n",
    "    cam.width = 2\n",
    "    cam.height = 2\n",
    "\n",
    "    dic = {\n",
    "        'ortho_ratio': 1,\n",
    "        'scale': 1,\n",
    "        'center': np.zeros(3),\n",
    "        'R' : np.eye(3)\n",
    "    }\n",
    "    calib = opengl_util.load_calib(dic, render_size=size)\n",
    "    extrinsic = calib[:4, :4]\n",
    "    intrinsic = calib[4:8, :4]\n",
    "    calib_mat = np.matmul(intrinsic, extrinsic)\n",
    "    cam.width = 2\n",
    "    cam.height = 2\n",
    "    cam.set_projection_matrix(calib_mat[:3,:4])\n",
    "\n",
    "    rndr = ColorRender(width=size, height=size, egl=egl)\n",
    "    rndr.set_mesh(\n",
    "        vertices, faces, normals, faces_normals\n",
    "    )\n",
    "\n",
    "    rndr.set_norm_mat(1, np.zeros(3))\n",
    "\n",
    "    rndr.set_camera(cam)\n",
    "    rndr.display()\n",
    "    depth = opengl_util.depth_render_result(rndr, output_path)\n",
    "    return depth\n",
    "\n",
    "def invert_projection(depth):\n",
    "\n",
    "    mask = depth[:, :, -1] == 1\n",
    "    z_ndc = depth[:, :, 0]\n",
    "\n",
    "    z_ndc = z_ndc[mask]\n",
    "    x = np.linspace(-1, 1, 512)\n",
    "    y = np.linspace(-1, 1, 512)\n",
    "\n",
    "    xv, yv = np.meshgrid(x,y)\n",
    "    x_ndc = xv[mask]\n",
    "    y_ndc = yv[mask]\n",
    "\n",
    "    y_ndc = -y_ndc\n",
    "    z_ndc = -z_ndc\n",
    "    \n",
    "    pcls = np.stack([x_ndc, y_ndc, z_ndc], axis= -1)\n",
    "    pcls_view = pcls\n",
    "    return pcls_view\n",
    "\n",
    "def load_calib(calib_path):\n",
    "    calib_data = np.loadtxt(calib_path, dtype=float)\n",
    "    extrinsic = calib_data[:4, :4]\n",
    "    intrinsic = calib_data[4:8, :4]\n",
    "    calib_mat = np.matmul(intrinsic, extrinsic)\n",
    "    return calib_mat\n",
    "\n",
    "def get_proj_pcls(mesh_file, calib_path=None, z_norm=True):\n",
    "    vertices, faces, normals, faces_normals, _, _ = load_scan(\n",
    "        mesh_file, with_normal=True, with_texture=True\n",
    "    )\n",
    "    if calib_path is not None:\n",
    "        scale = 100.0\n",
    "        vertices *= scale\n",
    "        calib = load_calib(calib_path)\n",
    "        vertices = projection(vertices, calib)\n",
    "        vertices[:, 1] *= -1\n",
    "    if z_norm:\n",
    "        vertices[:, 2] -= vertices[:, 2].mean()\n",
    "\n",
    "    depth = get_depth_map(vertices, faces, normals, faces_normals)\n",
    "    proj_pcl = invert_projection(depth)\n",
    "\n",
    "    return proj_pcl, vertices, faces\n",
    "\n",
    "def projection(vertices, calib):\n",
    "    vertices = np.matmul(calib[:3, :3], vertices.T).T + calib[:3, 3]\n",
    "    \n",
    "    return vertices\n",
    "\n",
    "from typing import Union\n",
    "from pytorch3d.loss.chamfer import (\n",
    "    _handle_pointcloud_input,\n",
    "    _validate_chamfer_reduction_inputs,\n",
    "    knn_points\n",
    "    )\n",
    "\n",
    "def chamfer_distance(\n",
    "    x,\n",
    "    y,\n",
    "    x_lengths=None,\n",
    "    y_lengths=None,\n",
    "    x_normals=None,\n",
    "    y_normals=None,\n",
    "    weights=None,\n",
    "    batch_reduction: Union[str, None] = \"mean\",\n",
    "    point_reduction: str = \"mean\",\n",
    "    norm: int = 2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Chamfer distance between two pointclouds x and y.\n",
    "\n",
    "    Args:\n",
    "        x: FloatTensor of shape (N, P1, D) or a Pointclouds object representing\n",
    "            a batch of point clouds with at most P1 points in each batch element,\n",
    "            batch size N and feature dimension D.\n",
    "        y: FloatTensor of shape (N, P2, D) or a Pointclouds object representing\n",
    "            a batch of point clouds with at most P2 points in each batch element,\n",
    "            batch size N and feature dimension D.\n",
    "        x_lengths: Optional LongTensor of shape (N,) giving the number of points in each\n",
    "            cloud in x.\n",
    "        y_lengths: Optional LongTensor of shape (N,) giving the number of points in each\n",
    "            cloud in y.\n",
    "        x_normals: Optional FloatTensor of shape (N, P1, D).\n",
    "        y_normals: Optional FloatTensor of shape (N, P2, D).\n",
    "        weights: Optional FloatTensor of shape (N,) giving weights for\n",
    "            batch elements for reduction operation.\n",
    "        batch_reduction: Reduction operation to apply for the loss across the\n",
    "            batch, can be one of [\"mean\", \"sum\"] or None.\n",
    "        point_reduction: Reduction operation to apply for the loss across the\n",
    "            points, can be one of [\"mean\", \"sum\"].\n",
    "        norm: int indicates the norm used for the distance. Supports 1 for L1 and 2 for L2.\n",
    "\n",
    "    Returns:\n",
    "        2-element tuple containing\n",
    "\n",
    "        - **loss**: Tensor giving the reduced distance between the pointclouds\n",
    "          in x and the pointclouds in y.\n",
    "        - **loss_normals**: Tensor giving the reduced cosine distance of normals\n",
    "          between pointclouds in x and pointclouds in y. Returns None if\n",
    "          x_normals and y_normals are None.\n",
    "    \"\"\"\n",
    "    _validate_chamfer_reduction_inputs(batch_reduction, point_reduction)\n",
    "\n",
    "    if not ((norm == 1) or (norm == 2)):\n",
    "        raise ValueError(\"Support for 1 or 2 norm.\")\n",
    "\n",
    "    x, x_lengths, x_normals = _handle_pointcloud_input(x, x_lengths, x_normals)\n",
    "    y, y_lengths, y_normals = _handle_pointcloud_input(y, y_lengths, y_normals)\n",
    "\n",
    "    return_normals = x_normals is not None and y_normals is not None\n",
    "\n",
    "    N, P1, D = x.shape\n",
    "    P2 = y.shape[1]\n",
    "\n",
    "    # Check if inputs are heterogeneous and create a lengths mask.\n",
    "    is_x_heterogeneous = (x_lengths != P1).any()\n",
    "    is_y_heterogeneous = (y_lengths != P2).any()\n",
    "    x_mask = (\n",
    "        torch.arange(P1, device=x.device)[None] >= x_lengths[:, None]\n",
    "    )  # shape [N, P1]\n",
    "    y_mask = (\n",
    "        torch.arange(P2, device=y.device)[None] >= y_lengths[:, None]\n",
    "    )  # shape [N, P2]\n",
    "\n",
    "    if y.shape[0] != N or y.shape[2] != D:\n",
    "        raise ValueError(\"y does not have the correct shape.\")\n",
    "    if weights is not None:\n",
    "        if weights.size(0) != N:\n",
    "            raise ValueError(\"weights must be of shape (N,).\")\n",
    "        if not (weights >= 0).all():\n",
    "            raise ValueError(\"weights cannot be negative.\")\n",
    "        if weights.sum() == 0.0:\n",
    "            weights = weights.view(N, 1)\n",
    "            if batch_reduction in [\"mean\", \"sum\"]:\n",
    "                return (\n",
    "                    (x.sum((1, 2)) * weights).sum() * 0.0,\n",
    "                    (x.sum((1, 2)) * weights).sum() * 0.0,\n",
    "                )\n",
    "            return ((x.sum((1, 2)) * weights) * 0.0, (x.sum((1, 2)) * weights) * 0.0)\n",
    "\n",
    "    cham_norm_x = x.new_zeros(())\n",
    "    cham_norm_y = x.new_zeros(())\n",
    "\n",
    "    x_nn = knn_points(x, y, lengths1=x_lengths, lengths2=y_lengths, norm=norm, K=1)\n",
    "    y_nn = knn_points(y, x, lengths1=y_lengths, lengths2=x_lengths, norm=norm, K=1)\n",
    "    cham_x = x_nn.dists[..., 0]  # (N, P1)\n",
    "    cham_y = y_nn.dists[..., 0]  # (N, P2)\n",
    "    cham_x = torch.sqrt(cham_x)\n",
    "    cham_y = torch.sqrt(cham_y)\n",
    "    if is_x_heterogeneous:\n",
    "        cham_x[x_mask] = 0.0\n",
    "    if is_y_heterogeneous:\n",
    "        cham_y[y_mask] = 0.0\n",
    "\n",
    "    if weights is not None:\n",
    "        cham_x *= weights.view(N, 1)\n",
    "        cham_y *= weights.view(N, 1)\n",
    "\n",
    "    # Apply point reduction\n",
    "    cham_x = cham_x.sum(1)  # (N,)\n",
    "    cham_y = cham_y.sum(1)  # (N,)\n",
    "\n",
    "    if point_reduction == \"mean\":\n",
    "        x_lengths_clamped = x_lengths.clamp(min=1)\n",
    "        y_lengths_clamped = y_lengths.clamp(min=1)\n",
    "        cham_x /= x_lengths_clamped\n",
    "        cham_y /= y_lengths_clamped\n",
    "        if return_normals:\n",
    "            cham_norm_x /= x_lengths_clamped\n",
    "            cham_norm_y /= y_lengths_clamped\n",
    "\n",
    "    if batch_reduction is not None:\n",
    "        # batch_reduction == \"sum\"\n",
    "        cham_x = cham_x.sum()\n",
    "        cham_y = cham_y.sum()\n",
    "        if return_normals:\n",
    "            cham_norm_x = cham_norm_x.sum()\n",
    "            cham_norm_y = cham_norm_y.sum()\n",
    "        if batch_reduction == \"mean\":\n",
    "            div = weights.sum() if weights is not None else max(N, 1)\n",
    "            cham_x /= div\n",
    "            cham_y /= div\n",
    "            if return_normals:\n",
    "                cham_norm_x /= div\n",
    "                cham_norm_y /= div\n",
    "\n",
    "    cham_dist = cham_x + cham_y\n",
    "    cham_normals = cham_norm_x + cham_norm_y if return_normals else None\n",
    "\n",
    "    return cham_dist, cham_normals\n",
    "\n",
    "from pytorch3d.loss.point_mesh_distance import _PointFaceDistance\n",
    "def point_mesh_distance(meshes, pcls):\n",
    "    # source code of pytorch3D.loss.point_mesh_face_distance\n",
    "    \n",
    "    if len(meshes) != len(pcls):\n",
    "        raise ValueError(\"meshes and pointclouds must be equal sized batches\")\n",
    "    N = len(meshes)\n",
    "\n",
    "    # packed representation for pointclouds\n",
    "    points = pcls.points_packed()    # (P, 3)\n",
    "    points_first_idx = pcls.cloud_to_packed_first_idx()\n",
    "    max_points = pcls.num_points_per_cloud().max().item()\n",
    "\n",
    "    # packed representation for faces\n",
    "    verts_packed = meshes.verts_packed() # (sum(V_n), 3)\n",
    "    faces_packed = meshes.faces_packed() # (sum(F_n), 3)\n",
    "    tris = verts_packed[faces_packed]    # (T, 3, 3)\n",
    "    tris_first_idx = meshes.mesh_to_faces_packed_first_idx()\n",
    "\n",
    "    # point to face distance: shape (P,)\n",
    "    point_to_face = _PointFaceDistance.apply(\n",
    "        points, points_first_idx, tris, tris_first_idx, max_points, 5e-3\n",
    "    )\n",
    "\n",
    "    # weight each example by the inverse of number of points in the example\n",
    "    point_to_cloud_idx = pcls.packed_to_cloud_idx()    # (sum(P_i),)\n",
    "    num_points_per_cloud = pcls.num_points_per_cloud()    # (N,)\n",
    "    weights_p = num_points_per_cloud.gather(0, point_to_cloud_idx)\n",
    "    weights_p = 1.0 / weights_p.float()\n",
    "    point_to_face = torch.sqrt(point_to_face) * weights_p\n",
    "    point_dist = point_to_face.sum() / N\n",
    "\n",
    "    return point_dist\n",
    "\n",
    "def get_p2s_distance(vertices, faces, pcls):\n",
    "    from lib.common.render import Render\n",
    "    render = Render(size=512, device=device)\n",
    "    mesh = render.VF2Mesh(vertices, faces)\n",
    "    pcls = torch.from_numpy(pcls).float()\n",
    "    # for single pcl\n",
    "    pcl = Pointclouds(torch.unsqueeze(pcls, 0))\n",
    "    pcl = pcl.to(device)\n",
    "    p2s_dist = point_mesh_distance(mesh, pcl) * 100\n",
    "    return p2s_dist\n",
    "\n",
    "def calc_metrics(gt_mesh_path, pred_mesh_path, calib_path, z_norm=True):\n",
    "    gt_pcl, gt_vertices, gt_faces = get_proj_pcls(gt_mesh_path, calib_path, z_norm)\n",
    "    pred_pcl, pred_vertices, pred_faces = get_proj_pcls(pred_mesh_path,z_norm=z_norm)\n",
    "    \n",
    "    # proj point clouds chamfer\n",
    "    chamfer_dist = get_chamfer_distance(gt_pcl, pred_pcl)\n",
    "    \n",
    "    # P2S (proj pcls to gt mesh)\n",
    "    p2s_dist = get_p2s_distance(gt_vertices, gt_faces, pred_pcl)\n",
    "    return chamfer_dist, p2s_dist\n",
    "\n",
    "\n",
    "import trimesh\n",
    "def export_obj(vertices, faces, out_path, vertiex_colors=None):\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    trimesh.Trimesh(vertices, faces, vertex_colors=vertiex_colors).export(out_path)\n",
    "\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.ops import sample_points_from_meshes\n",
    "from lib.common.render import Render\n",
    "\n",
    "def get_icon_chamfer(tgt_mesh, pred_mesh, num_sample_points=40000):\n",
    "    tgt_points = Pointclouds(sample_points_from_meshes(tgt_mesh, num_sample_points))\n",
    "    pred_points = Pointclouds(sample_points_from_meshes(pred_mesh, num_sample_points))\n",
    "    invert_p2s = point_mesh_distance(tgt_mesh, pred_points) * 100 \n",
    "    p2s_dist = point_mesh_distance(pred_mesh, tgt_points) * 100 \n",
    "    chamfer_dist = (invert_p2s + p2s_dist) / 2\n",
    "    # chamfer_dist = pytorch3d.loss.chamfer_distance(tgt_points, pred_points)[0] * 100\n",
    "    return p2s_dist, chamfer_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    OrthographicCameras,\n",
    "    PointsRasterizationSettings,\n",
    "    PointsRenderer,\n",
    "    PointsRasterizer,\n",
    "    AlphaCompositor,\n",
    ")\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "# Setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "R, T = look_at_view_transform(20, 0, 0)\n",
    "cameras = OrthographicCameras(device=device, R=R, T=T)\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "    image_size=512, \n",
    "    radius = 0.003,\n",
    "    points_per_pixel = 10\n",
    ")\n",
    "\n",
    "rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "renderer = PointsRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    compositor=AlphaCompositor(background_color=(0, 0, 0))\n",
    ")\n",
    "\n",
    "def get_pcls_normal_map(renderer, pcl):\n",
    "    pcl_tensor = torch.from_numpy(pcl).float().to(device)\n",
    "    normals = estimate_normals(pcl_tensor)\n",
    "    normals = test_normals(normals)\n",
    "    verts = pcl_tensor.to(device)\n",
    "    rgb = normals.to(device)\n",
    "    gt_pcls = Pointclouds(points=[verts], features=[rgb])\n",
    "    images = renderer(gt_pcls)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load calib file \n",
    "calib_paths = {\n",
    "    '000': [],\n",
    "    '120': [],\n",
    "    '240': []\n",
    "}\n",
    "\n",
    "for path in subject_paths:\n",
    "    for key in calib_paths.keys():\n",
    "        calib_path = os.path.join(path, f'calib/{key}.txt')\n",
    "        calib_paths[key].append(calib_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for calib, pred in zip(calib_paths.keys(), pred_paths.keys()):\n",
    "    print(calib, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc chamfer score and p2s score \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm \n",
    "score = {\n",
    "\n",
    "}\n",
    "z_norm = False\n",
    "for calib_key, pred_key in zip(calib_paths.keys(), pred_paths.keys()):\n",
    "    calib_path_list = calib_paths[calib_key]\n",
    "    pred_path_list = pred_paths[pred_key]\n",
    "    for gt, pred, calib in zip(gt_scan_paths, pred_path_list, calib_path_list):\n",
    "        gt_subject_name = os.path.basename(gt)[:-4]\n",
    "        pred_subject_name = pred.split('/')[-2]\n",
    "        calib_subject_name = calib.split('/')[-3]\n",
    "        if gt_subject_name == pred_subject_name and pred_subject_name == calib_subject_name:\n",
    "\n",
    "            gt_pcl, gt_vertices, gt_faces = get_proj_pcls(gt, calib, z_norm)\n",
    "            pred_pcl, pred_vertices, pred_faces = get_proj_pcls(pred, z_norm=z_norm)\n",
    "            chamfer_dist = get_chamfer_distance(gt_pcl, pred_pcl)\n",
    "\n",
    "            # # P2S (proj pcls to gt mesh)\n",
    "            p2s_dist = get_p2s_distance(gt_vertices, gt_faces, pred_pcl)\n",
    "            gt_images = get_pcls_normal_map(renderer, gt_pcl)\n",
    "            pred_images = get_pcls_normal_map(renderer, pred_pcl)\n",
    "            normal_score = ((gt_images[0, ..., :3] - pred_images[0, ..., :3]) ** 2).sum(-1).mean()\n",
    "            tmp_dict = {\n",
    "                'chamfer': chamfer_dist.cpu().numpy(),\n",
    "                'p2s': p2s_dist.cpu().numpy(),\n",
    "                'normal_mse': normal_score.cpu().numpy(),\n",
    "            }\n",
    "            print(chamfer_dist, p2s_dist, normal_score)\n",
    "            score[gt_subject_name] = tmp_dict\n",
    "        break\n",
    "    break\n",
    "# np.save('cape_score.npy', score, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(gt_images[0, ..., :3].cpu().numpy())\n",
    "plt.show()\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pred_images[0, ..., :3].cpu().numpy())\n",
    "plt.show()\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "dat = np.load('cape_score_wo_norm.npy', allow_pickle=True).tolist()\n",
    "df = pd.DataFrame(dat)\n",
    "# df.to_csv('cape_score.csv', index=False, header=True)\n",
    "len(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_path = \"/mnt/local4T/pengfei/projects/PointHuman/PointHuman-ICON/data/cape/test150.txt\"\n",
    "\n",
    "data = []\n",
    "with open(split_path, 'r', encoding='utf-8') as f:\n",
    "    for ann in f.readlines():\n",
    "        ann = ann.strip('\\n')       #去除文本中的换行符\n",
    "        ann = ann.split('/')[1]\n",
    "        data.append(ann)\n",
    "simple_subject_name = data[:50]\n",
    "hard_subject_name = data[50:]\n",
    "\n",
    "len(simple_subject_name), len(hard_subject_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cape_easy': {'chamfer': 2.0671952311197916,\n",
       "  'p2s': 1.0668431599934896,\n",
       "  'normal': 0.009016210238138835},\n",
       " 'cape_hard': {'chamfer': 2.3452838134765623,\n",
       "  'p2s': 1.203035888671875,\n",
       "  'normal': 0.009626408418019613}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_df = df.loc[df['subject_name'].isin(simple_subject_name)]\n",
    "hard_df = df.loc[df['subject_name'].isin(hard_subject_name)]\n",
    "\n",
    "{\n",
    "    'cape_easy':{\n",
    "        'chamfer': easy_df.chamfer.mean(),\n",
    "        'p2s': easy_df.p2s.mean(),\n",
    "        'normal': easy_df.normal_mse.mean()\n",
    "    },\n",
    "    'cape_hard':{\n",
    "        'chamfer': hard_df.chamfer.mean(),\n",
    "        'p2s': hard_df.p2s.mean(),\n",
    "        'normal': hard_df.normal_mse.mean()\n",
    "    }\n",
    "\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pointhuman-torch1.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
